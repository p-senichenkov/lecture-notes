\begin{definition}
	\it{Ускорение}, получаемое при использовании параллельного алгоритма для $ p $ вычислителей, по сравнению с последовательным вариантом выполнения вычислений, определяется величиной
	$$ S_p(n) = \frac{T_1(n)}{T_p(n)} $$
\end{definition}

\begin{definition}
	\it{Эффективность} использования параллельным алгоритмом вычислителей при решении задачи определяется соотношением
	$$ E_p(n) = \frac{T_1(n)}{pT_p(n)} = \frac{S_p(n)}p $$
\end{definition}

\begin{remark}
	\it{Сверхлинейное} ускорение $ S_p > p $ может иметь место в силу неравноправности выполнения последовательной и параллельной программ, нелинейного характера зависимости сложности от объёма обрабатываемых данных, и т. д.
\end{remark}

\begin{definition}
	\it{Стоимость} вычислений определяется как произведение времени параллельного решения задачи и числа используемых вычислителей
	$$ C_p = pT_p $$
\end{definition}

\subsection*{Закон Амдаля (Amdahl)}

Достижению максимального ускорения может препятствовать существование в выполняемых вычислениях последовательных расчётов, которые не могут быть распараллелены.

Пусть $ f $ ~--- доля последовательных вычислений в применяемом алгоритме обработки данных.

Ускорение процесса вычислений при использовании $ p $ вычислитетлей оценивается величиной:
$$ S_p = \frac{T_1}{T_p} = \frac{f + (1 - f)}{f + \frac{1 - f}p} \le S^* = \frac1f $$

\subsection*{Закон Густавсона--Барсиса}

Оценим максимально достижимое ускорение исходя из имеющейся доли послеовательных расчётов в выполняемых параллельных вычислениях:
$$ g = \frac{\tau(n)}{\tau(n) + \frac{\pi(n)}p}, $$
где $ \tau(n) $ и $ \pi(n) $ -- времена последовательной и параллельной частей, \ie
$$ T_1 = \tau(n) + \pi(n), \qquad T_p = \tau(n) + \frac{\pi(n)}p $$
С учётом величины $ g $ можно получить
$$ \tau(n) = g \bigg( \tau(n) + \frac{\pi(n)}p \bigg), \qquad \pi(n) = (1 - g)p \bigg( \tau(n) + \frac{\pi(n)}p \bigg), $$
что позволяет получить оценку для ускорения
$$ S_p = \frac{T_1}{T_p} = \frac{\tau(n) + \pi(n)}{\tau(n) + \frac{\pi(n)}p} = \frac{ \bigg( \tau(n) + \frac{\pi(n)}p \bigg) \bigg( g + (1 - g)p \bigg)}{\tau(n) + \frac{\pi(n)}p} $$

\begin{definition}
	Параллельный алгоритм называют \it{масштабируемым}, если при росте числа вычислителей он обеспечивает увеличение ускорения при сохранении постоянного уровня эффективности использования процессоров.
\end{definition}

\begin{definition}
	\it{Накладные расходы} при выполнении параллельного алгоритма появляются за счёт необходимости организации взаимодействия вычислителей, выполнения некоторых дополнительных действий, синхронизации параллельных вычислений, и т. д.
\end{definition}

Пусть $ E = \const $ ~--- желаемый уровень эффективности выполняемых вычислений. Из выражения для эффективности можно получить
$$ \frac{T_0}{T_1} = \frac{1 - E}E, \qquad T_1 = \frac E{1 - E} \cdot T_0 $$

Порождаемую этим соотношениум зависимость $ n = F(p) $ обычно называют \it{функцией изоэффективности}.
